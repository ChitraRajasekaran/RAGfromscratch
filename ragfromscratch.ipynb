{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "661be8fd-0703-4307-bfdd-63c5b1a2a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ccf3f77-512f-4917-9e51-836b5853a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96461f73-24a8-42d1-866f-930f68340a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae747de-000f-491b-82ed-94579caaebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9013e44-fb5c-493e-9ca1-c6846b1a54ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps, allowing for easier execution and understanding. It can be achieved through methods such as prompting with specific instructions, human inputs, or utilizing external classical planners for long-horizon planning. Task decomposition helps in transforming big tasks into manageable ones, enhancing model performance and interpretation of the thinking process.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4 #html parsing\n",
    "from langchain import hub # fetches prebuilt prompt from langchain hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # splits long texts into chunks\n",
    "from langchain_community.document_loaders import WebBaseLoader # loads web pages as LangChain documents\n",
    "from langchain_community.vectorstores import Chroma # a local vector store db\n",
    "from langchain_core.output_parsers import StrOutputParser #converts llm responses into plain strings\n",
    "from langchain_core.runnables import RunnablePassthrough # passess the original output through a chain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings #chatllmwrapper #text to embedding vectors\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents: fetches the blog post and keeps only yhe element from class post-content,title and header\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split into chunks : break 1000 char chunks and 200 char overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and store in vector db : openaiembedding turns chunk into vector, chroma.from_docs saves in local chroma dir(in memoery unless given a persist dir)\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever() # gives retriever interface that does similarity search with top k matches\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt \n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing : concatenates retrieved chunks into one context for the prompt\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920df03-e424-4e8e-a71f-fd60b4043a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gpt",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
